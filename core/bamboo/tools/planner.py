"""LLM-backed planner tool.

This module provides an MCP tool that asks an LLM to return a *machine-parseable*
execution plan describing which Bamboo tool(s) to call for a given user question.

The plan is validated with Pydantic to ensure:
  * a stable JSON shape,
  * type correctness,
  * lightweight semantic constraints (e.g., confidence in [0, 1]).

The planner is intentionally **separate** from the main answer/orchestration
tooling so that Bamboo can implement a hybrid strategy:
  * Use deterministic routing for obvious cases.
  * Fall back to LLM planning when intent is ambiguous or multi-step.

Note:
  The planner only returns a plan. It does not execute tools.
"""

from __future__ import annotations

import json
import re
from enum import Enum
from typing import Any

from pydantic import BaseModel, Field, model_validator

from bamboo.llm.runtime import get_llm_manager, get_llm_selector
from bamboo.llm.types import GenerateParams, Message
from bamboo.tools.base import MCPContent, text_content
from bamboo.tools.loader import list_tool_entry_points, find_tool_by_name


class PlanRoute(str, Enum):
    """Routing decision returned by the planner."""

    FAST_PATH = "FAST_PATH"
    PLAN = "PLAN"
    RETRIEVE = "RETRIEVE"


class ToolCall(BaseModel):
    """A single tool invocation proposed by the planner."""

    tool: str = Field(
        ..., min_length=1, description="Tool name as used by Bamboo (e.g., 'panda_task_status' or 'atlas.task_status')."
    )
    arguments: dict[str, Any] = Field(default_factory=dict, description="JSON arguments to pass to the tool.")
    namespace: str | None = Field(
        default=None,
        description=(
            "Optional namespace hint used when resolving tools via entry points (e.g., 'atlas'). "
            "If omitted, the executor may resolve by suffix/name."
        ),
    )


class RetrievalQuery(BaseModel):
    """Optional retrieval hint (future-proof; DB/vector store may implement this)."""

    type: str = Field(
        ..., description="Retrieval method (e.g., 'exact', 'by_entity', 'embedding')."
    )
    keys: dict[str, Any] = Field(default_factory=dict, description="Structured keys (e.g., task_id, job_id, error_signature).")


class ReusePolicy(BaseModel):
    """Policy hints about reusing past answers/evidence."""

    allow_final_answer_reuse: bool = Field(
        default=False,
        description="If true, a previously stored final answer may be returned without fresh tool calls (use carefully).",
    )
    allow_pattern_reuse: bool = Field(
        default=True,
        description="If true, previously observed troubleshooting patterns may be reused as suggestions.",
    )
    requires_fresh_evidence: bool = Field(
        default=True,
        description="If true, the executor should obtain fresh tool evidence before presenting a final answer.",
    )


class Plan(BaseModel):
    """Top-level plan object returned by the planner."""

    route: PlanRoute = Field(..., description="How the executor should proceed: FAST_PATH, PLAN, or RETRIEVE.")
    confidence: float = Field(
        ..., ge=0.0, le=1.0, description="Planner confidence in [0, 1]."
    )
    tool_calls: list[ToolCall] = Field(
        default_factory=list,
        description="Ordered list of tool calls to execute. Empty is allowed for RETRIEVE-only decisions.",
    )
    retrieval_query: RetrievalQuery | None = Field(
        default=None,
        description="Optional retrieval instruction. Not required until DB/vector store integration.",
    )
    reuse_policy: ReusePolicy = Field(
        default_factory=ReusePolicy,
        description="Hints controlling reuse of past answers/patterns.",
    )
    explain: str = Field(
        default="",
        max_length=1000,
        description="Short human-readable rationale for debugging and test traces.",
    )

    @model_validator(mode="after")
    def _check_semantics(self) -> "Plan":
        """Validate cross-field semantics."""
        if self.route in (PlanRoute.FAST_PATH, PlanRoute.PLAN) and not self.tool_calls:
            raise ValueError("route FAST_PATH/PLAN requires at least one tool_call")
        if self.route == PlanRoute.RETRIEVE and self.retrieval_query is None and not self.tool_calls:
            raise ValueError("route RETRIEVE requires retrieval_query and/or tool_calls")
        return self


def get_plan_json_schema() -> dict[str, Any]:
    """Return the exact JSON schema for the :class:`Plan` object.

    Returns:
        Dict[str, Any]: JSON Schema (draft-2020-12 compatible) generated by Pydantic.
    """
    return Plan.model_json_schema()


_JSON_FENCE_RE = re.compile(r"```(?:json)?\s*(\{.*?\})\s*```", re.DOTALL | re.IGNORECASE)


def extract_first_json_object(text: str) -> str:
    """Extract the first JSON object from an LLM response.

    The planner prompt requests *JSON only*, but models sometimes wrap output
    in code fences or include leading commentary. This helper extracts the
    first JSON object conservatively.

    Args:
        text: Raw model response text.

    Returns:
        str: A JSON object string.

    Raises:
        ValueError: If no JSON object could be extracted.
    """
    if not text:
        raise ValueError("Empty response")

    m = _JSON_FENCE_RE.search(text)
    if m:
        return m.group(1).strip()

    # Fallback: find first '{' and attempt to parse the largest valid object.
    start = text.find("{")
    if start < 0:
        raise ValueError("No JSON object start found")

    candidate = text[start:].strip()
    # Try progressively shorter suffixes to find a valid JSON object.
    for end in range(len(candidate), 1, -1):
        snippet = candidate[:end]
        try:
            json.loads(snippet)
            return snippet
        except Exception:  # pylint: disable=broad-exception-caught
            continue
    raise ValueError("Unable to extract valid JSON object")


def build_planner_system_prompt(schema: dict[str, Any]) -> str:
    """Build the system prompt for the planner.

    Args:
        schema: JSON schema that the model output must conform to.

    Returns:
        str: System prompt text.
    """
    schema_compact = json.dumps(schema, ensure_ascii=False)
    return (
        "You are a tool planner for an MCP server. "
        "Your job is to output a single JSON object that conforms exactly to the provided JSON Schema.\n\n"
        "Hard rules:\n"
        "- Output MUST be valid JSON (no trailing commas).\n"
        "- Output MUST be a single JSON object, and MUST NOT be wrapped in markdown fences.\n"
        "- Do not include any explanation outside the JSON object.\n"
        "- Only propose tools that appear in the provided tool catalog.\n"
        "- Be conservative: if uncertain, set route='PLAN' and confidence lower.\n\n"
        f"JSON Schema (must match exactly):\n{schema_compact}\n"
    )


def build_planner_user_prompt(
    question: str,
    tool_catalog: list[dict[str, Any]],
    hints: dict[str, Any] | None = None,
) -> str:
    """Build the user prompt for the planner.

    Args:
        question: User question.
        tool_catalog: List of tool definition objects (name, description, inputSchema).
        hints: Optional structured hints from deterministic extraction.

    Returns:
        str: User prompt text.
    """
    payload = {
        "question": question,
        "tool_catalog": tool_catalog,
    }
    if hints:
        payload["hints"] = hints
    return (
        "Given the user's question, choose the best route and propose tool calls. "
        "Use the hints if they are present.\n\n"
        "Return only the JSON plan.\n\n"
        f"Input:\n{json.dumps(payload, ensure_ascii=False)}"
    )


def _collect_tool_catalog(namespaces: list[str] | None = None) -> list[dict[str, Any]]:
    """Collect a compact tool catalog for the planner.

    Args:
        namespaces: Optional list of namespaces to include (e.g. ['atlas']). If
            omitted, include all discovered entry points.

    Returns:
        List[Dict[str, Any]]: Tool definitions suitable for prompt inclusion.
    """
    out: list[dict[str, Any]] = []
    for ep in list_tool_entry_points():
        name = ep.get("name", "")
        if namespaces:
            ns = name.split(".", 1)[0] if "." in name else ""
            if ns not in namespaces:
                continue

        # Load tool object via loader resolution and call get_definition().
        if "." in name:
            ns, _, tool_name = name.partition(".")
            resolved = find_tool_by_name(tool_name, namespace=ns)
        else:
            resolved = find_tool_by_name(name)
        if not resolved or not getattr(resolved.obj, "get_definition", None):
            continue

        try:
            d = resolved.obj.get_definition()  # type: ignore[attr-defined]
        except Exception:  # pylint: disable=broad-exception-caught
            continue

        # Keep only the fields the planner needs.
        out.append(
            {
                "name": d.get("name") or name,
                "description": d.get("description", ""),
                "inputSchema": d.get("inputSchema", {}),
            }
        )
    return out


class BambooPlannerTool:
    """LLM-backed planner that outputs a JSON plan."""

    @staticmethod
    def get_definition() -> dict[str, Any]:
        """Return the MCP tool discovery definition.

        Returns:
            Dict[str, Any]: Tool definition compatible with MCP discovery.
        """
        return {
            "name": "bamboo_plan",
            "description": (
                "LLM-backed planner that returns a JSON execution plan (route + tool_calls). "
                "Use it when deterministic routing is uncertain or when a multi-step plan is required."
            ),
            "inputSchema": {
                "type": "object",
                "properties": {
                    "question": {"type": "string", "description": "User question to plan for."},
                    "hints": {
                        "type": "object",
                        "description": "Optional structured hints from deterministic extraction.",
                    },
                    "namespaces": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Optional list of namespaces to include in the tool catalog (e.g. ['atlas']).",
                    },
                    "temperature": {"type": "number", "default": 0.0, "description": "Planner temperature (keep low)."},
                    "max_tokens": {"type": "integer", "default": 900, "description": "Max completion tokens."},
                },
                "required": ["question"],
            },
        }

    async def call(self, arguments: dict[str, Any]) -> list[MCPContent]:
        """Execute the planner call and return the validated plan as JSON text.

        Args:
            arguments: Tool arguments.

        Returns:
            List[Dict[str, Any]]: Single text content block containing the JSON plan.

        Raises:
            ValueError: If the question is missing or the model output cannot be validated.
            RuntimeError: If the LLM runtime is not initialized.
        """
        question = str(arguments.get("question", "") or "").strip()
        if not question:
            raise ValueError("'question' is required")

        namespaces = arguments.get("namespaces")
        namespaces_list = [str(x) for x in namespaces] if isinstance(namespaces, list) else None

        hints = arguments.get("hints")
        hints_dict = hints if isinstance(hints, dict) else None

        schema = get_plan_json_schema()
        tool_catalog = _collect_tool_catalog(namespaces=namespaces_list)

        system = build_planner_system_prompt(schema)
        user = build_planner_user_prompt(question=question, tool_catalog=tool_catalog, hints=hints_dict)
        messages: list[Message] = [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ]

        temperature = float(arguments.get("temperature", 0.0))
        max_tokens = int(arguments.get("max_tokens", 900))

        text = await _call_default_llm(messages, temperature=temperature, max_tokens=max_tokens)

        # Validate, with one optional repair attempt.
        try:
            plan = Plan.model_validate_json(extract_first_json_object(text))
        except Exception as e:  # pylint: disable=broad-exception-caught
            repair_messages = messages + [
                {
                    "role": "user",
                    "content": (
                        "Your previous output was invalid. "
                        "Return ONLY a corrected JSON object that matches the schema exactly.\n"
                        f"Validation error: {str(e)[:300]}"
                    ),
                }
            ]
            text2 = await _call_default_llm(repair_messages, temperature=0.0, max_tokens=max_tokens)
            try:
                plan = Plan.model_validate_json(extract_first_json_object(text2))
            except Exception as e2:  # pylint: disable=broad-exception-caught
                raise ValueError(f"Planner output did not validate after repair attempt: {e2}") from e2

        # Pydantic's JSON helpers intentionally limit json.dumps kwargs.
        # Use model_dump() + json.dumps to keep Unicode readable.
        return text_content(json.dumps(plan.model_dump(), indent=2, ensure_ascii=False))


async def _call_default_llm(messages: list[Message], temperature: float, max_tokens: int) -> str:
    """Call the configured default LLM profile.

    Args:
        messages: Chat messages.
        temperature: Sampling temperature.
        max_tokens: Maximum completion tokens.

    Returns:
        str: Raw model text response.
    """
    selector = get_llm_selector()
    manager = get_llm_manager()

    default_profile = getattr(selector, "default_profile", "default")
    registry = getattr(selector, "registry", None)
    if registry is None:
        raise RuntimeError("LLM selector does not expose a registry.")

    model_spec = registry.get(default_profile)
    client = await manager.get_client(model_spec)
    resp = await client.generate(
        messages=messages,
        params=GenerateParams(temperature=temperature, max_tokens=max_tokens),
    )
    return resp.text


bamboo_plan_tool = BambooPlannerTool()
